{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618085ca-191e-4362-93ff-19447d56c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8747f15-9a02-466f-8536-ae4e651b7b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"Scikit-learn version:\", sklearn.__version__)\n",
    "print(\"Numpy version:\", np.__version__)\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1333cab2-d61b-4024-9c3e-55594f5a4957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settingup and Cloning Repositories\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Set up paths\n",
    "base_dir = Path(\"/home/jovyan/transfer_learning\")\n",
    "nepali_data_dir = base_dir / \"data/nepali-data\"\n",
    "hindi_data_dir = base_dir / \"data/hindi-data\"\n",
    "\n",
    "# Clone repositories\n",
    "!git clone https://github.com/oya163/nepali-ner.git --quiet && \\\n",
    " git clone https://github.com/cfiltnlp/HiNER.git --quiet\n",
    "\n",
    "# Create directories\n",
    "nepali_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "hindi_data_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2932fd0a-6ad8-4103-a11d-99ee79f6858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy files\n",
    "shutil.copy(base_dir / \"nepali-ner/data/ebiquity_v2/stemmed/total.bio\", nepali_data_dir / \"nepali.txt\")\n",
    "\n",
    "for set_type in [\"train\", \"test\", \"validation\"]:\n",
    "    shutil.copy(base_dir / f\"HiNER/data/collapsed/{set_type}.conll\", hindi_data_dir / f\"{set_type}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c66ada-3bf8-4e66-a30f-2ec657d6c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming labels in the Nepali dataset to make it compatible with the Hindi dataset.\n",
    "def replace_labels_in_file(input_path, output_path, label_mapping):\n",
    "    with input_path.open('r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    updated_lines = [\n",
    "        ' '.join([parts[0], label_mapping.get(parts[1], parts[1])]) if len(parts) > 1 else ''\n",
    "        for line in lines if (parts := line.strip().split())\n",
    "    ]\n",
    "\n",
    "    with output_path.open('w', encoding='utf-8') as new_file:\n",
    "        new_file.write('\\n'.join(updated_lines) + '\\n')\n",
    "\n",
    "label_mapping = {\n",
    "    'B-LOC': 'B-LOCATION',\n",
    "    'B-ORG': 'B-ORGANIZATION',\n",
    "    'B-PER': 'B-PERSON',\n",
    "    'I-LOC': 'I-LOCATION',\n",
    "    'I-ORG': 'I-ORGANIZATION',\n",
    "    'I-PER': 'I-PERSON'\n",
    "}\n",
    "\n",
    "replace_labels_in_file(nepali_data_dir / \"nepali.txt\", \n",
    "                       nepali_data_dir / \"nepali_label_matched.txt\", \n",
    "                       label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7834f2f-f29a-49ce-9d58-d78f4f85b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Loading and reading the content of the uploaded Nepali NER tagged text file\n",
    "file_path = nepali_data_dir / \"nepali_label_matched.txt\"\n",
    "\n",
    "# Reading the content of the file to check its structure\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Function to parse sentences and their labels from the file\n",
    "def parse_sentences(lines):\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "    for line in lines:\n",
    "        if line.strip() == \"\":\n",
    "            if current_sentence:\n",
    "                sentences.append(current_sentence)\n",
    "                current_sentence = []\n",
    "        else:\n",
    "            word, tag = line.strip().split()\n",
    "            current_sentence.append((word, tag))\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "    return sentences\n",
    "\n",
    "# Parsing sentences\n",
    "sentences = parse_sentences(lines)\n",
    "\n",
    "# Function to distribute sentences into train, test, and validation sets based on label distribution\n",
    "def distribute_sentences(sentences, train_ratio=0.7, test_ratio=0.2, valid_ratio=0.1):\n",
    "    # Initialize distribution dictionaries\n",
    "    label_sentences = defaultdict(list)\n",
    "\n",
    "    # Collect sentences by label\n",
    "    for sentence in sentences:\n",
    "        label_set = set()\n",
    "        for _, tag in sentence:\n",
    "            if tag != 'O':\n",
    "                label_set.add(tag)\n",
    "\n",
    "        if not label_set:\n",
    "            label_set = {'O'}  # Pure 'O' sentences categorized under 'O'\n",
    "        for label in label_set:\n",
    "            label_sentences[label].append(sentence)\n",
    "\n",
    "    # Split into train, test, validation sets\n",
    "    train, test, valid = [], [], []\n",
    "    for label, sents in label_sentences.items():\n",
    "        random.shuffle(sents)\n",
    "        n_total = len(sents)\n",
    "        n_train = int(n_total * train_ratio)\n",
    "        n_test = int(n_total * test_ratio)\n",
    "\n",
    "        train.extend(sents[:n_train])\n",
    "        test.extend(sents[n_train:n_train + n_test])\n",
    "        valid.extend(sents[n_train + n_test:])\n",
    "\n",
    "    # Shuffling the datasets to ensure randomness\n",
    "    random.shuffle(train)\n",
    "    random.shuffle(test)\n",
    "    random.shuffle(valid)\n",
    "\n",
    "    return train, test, valid\n",
    "\n",
    "# Distribute the sentences\n",
    "train_set, test_set, validation_set = distribute_sentences(sentences)\n",
    "\n",
    "# Function to write datasets to files with tab-separated values\n",
    "def write_to_file(sentences, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        for sentence in sentences:\n",
    "            for word, label in sentence:\n",
    "                file.write(f\"{word}\\t{label}\\n\")  # Using a tab instead of space to separate word and label\n",
    "            file.write(\"\\n\")  # New line for each sentence\n",
    "\n",
    "# Write the train, test, and validation datasets to files\n",
    "write_to_file(train_set, nepali_data_dir / 'train.txt')\n",
    "write_to_file(test_set, nepali_data_dir / 'test.txt')\n",
    "write_to_file(validation_set, nepali_data_dir / 'validation.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cbeb0b-2f59-4afc-a98e-3bafecf8057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######Experiments##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2921fb-933e-44d9-a81f-ca15fa282cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monolingual NER on Hindi and Nepali datasets\n",
    "model_names = [\n",
    "    'google/muril-base-cased', \n",
    "    'distilbert/distilbert-base-multilingual-cased', \n",
    "    'google-bert/bert-base-multilingual-cased', \n",
    "    'google/rembert'\n",
    "]\n",
    "\n",
    "datasets = ['hindi', 'nepali']\n",
    "\n",
    "for DATA in datasets:\n",
    "    for MODEL_NAME in model_names:\n",
    "        # Extracting the first word after '/' to create folder to store model and results\n",
    "        MODEL_TYPE = MODEL_NAME.split('/')[1].split('-')[0]\n",
    "\n",
    "        output_dir = f\"{base_dir}/output/{DATA}/{MODEL_TYPE}/output\"\n",
    "        logging_dir = f\"{base_dir}/output/{DATA}/{MODEL_TYPE}/logs\"\n",
    "        model_dir = f\"{base_dir}/output/{DATA}/{MODEL_TYPE}/model\"\n",
    "\n",
    "        # Create directories if they don't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(logging_dir, exist_ok=True)\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "        # Now run your command\n",
    "        os.system(f'''\n",
    "        python trainer.py \\\n",
    "        --train \"{locals()[f'{DATA}_data_dir']}/train.txt\" \\\n",
    "        --validation \"{locals()[f'{DATA}_data_dir']}/validation.txt\" \\\n",
    "        --test \"{locals()[f'{DATA}_data_dir']}/test.txt\" \\\n",
    "        --model_name \"{MODEL_NAME}\" \\\n",
    "        --output_dir \"{output_dir}\" \\\n",
    "        --logging_dir \"{logging_dir}\" \\\n",
    "        --save_pretrained \"{model_dir}\" \\\n",
    "        > \"{output_dir}/output.txt\"\n",
    "        ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce5510-ddfb-4f9a-8d0f-87b9a77ed37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-lingual NER on Hindi and Nepali datasets\n",
    "model_names = [\n",
    "    'google/muril-base-cased', \n",
    "    'distilbert/distilbert-base-multilingual-cased', \n",
    "    'google-bert/bert-base-multilingual-cased', \n",
    "    'google/rembert'\n",
    "]\n",
    "\n",
    "datasets = ['hindi', 'nepali']\n",
    "\n",
    "for TRAIN_DATA in datasets:\n",
    "    if TRAIN_DATA == 'hindi':\n",
    "        PRETRAINED_MODEL = 'nepali'\n",
    "        DATASET_TYPE = 'nepali_hindi'\n",
    "    elif TRAIN_DATA == 'nepali':\n",
    "        PRETRAINED_MODEL = 'hindi'\n",
    "        DATASET_TYPE = 'hindi_nepali'\n",
    "\n",
    "    for MODEL_NAME in model_names:\n",
    "        MODEL_TYPE = MODEL_NAME.split('/')[1].split('-')[0]\n",
    "\n",
    "        # Define the directories that need to be created\n",
    "        local_model_path = f\"{base_dir}/output/{PRETRAINED_MODEL}/{MODEL_TYPE}/model\"\n",
    "        output_dir = f\"{base_dir}/output/{DATASET_TYPE}/{MODEL_TYPE}/output\"\n",
    "        logging_dir = f\"{base_dir}/output/{DATASET_TYPE}/{MODEL_TYPE}/logs\"\n",
    "        model_dir = f\"{base_dir}/output/{DATASET_TYPE}/{MODEL_TYPE}/model\"\n",
    "\n",
    "        # Create directories if they don't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(logging_dir, exist_ok=True)\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "        # Now run the command\n",
    "        os.system(f'''\n",
    "        python trainer.py \\\n",
    "        --use_local_model True \\\n",
    "        --local_model_path \"{local_model_path}\" \\\n",
    "        --model_name \"{MODEL_NAME}\" \\\n",
    "        --output_dir \"{output_dir}\" \\\n",
    "        --logging_dir \"{logging_dir}\" \\\n",
    "        --save_pretrained \"{model_dir}\" \\\n",
    "        --train \"{locals()[f'{TRAIN_DATA}_data_dir']}/train.txt\" \\\n",
    "        --validation \"{locals()[f'{TRAIN_DATA}_data_dir']}/validation.txt\" \\\n",
    "        --test \"{locals()[f'{TRAIN_DATA}_data_dir']}/test.txt\" > \"{output_dir}/output.txt\"\n",
    "        ''')"
   ]
  }
 ],
 "metadata": {
  "jupyter_starters": {
   "starter": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
